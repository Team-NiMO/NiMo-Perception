#!/usr/bin/env python3

# -*- encoding: utf-8 -*-

import math
import os
import signal
import subprocess
from typing import Any

import cv2 as cv
import numpy as np
import open3d as o3d
import pyransac3d as pyrsc
import rospy
from cv_bridge import CvBridge
from geometry_msgs.msg import Point, Pose2D
from message_filters import ApproximateTimeSynchronizer
from sensor_msgs.msg import Image, CameraInfo
from sklearn.cluster import DBSCAN
from termcolor import colored

from stalk_detect.config import (BEST_STALK_ALGO, DEPTH_SCALE, DEPTH_TOPIC,
                                 DEPTH_TRUNC, DRIVER_COMMAND, GRASP_POINT_ALGO, GRIPPER_WIDTH, IMAGE_TOPIC,
                                 INLIER_THRESHOLD, MAX_RANSAC_ITERATIONS,
                                 MAX_X, MAX_Y, MIN_X, MIN_Y, MINIMUM_MASK_AREA,
                                 OPTIMAL_STALK_DISTANCE, RUN_REALSENSE_ON_REQUEST, VISUALIZE,
                                 BestStalkOptions, GraspPointFindingOptions, CAMERA_INFO, GRIPPER_LENGTH_PAST_STALK)
from stalk_detect.model import MaskRCNN
from stalk_detect.srv import GetStalk, GetStalkRequest, GetStalkResponse
from stalk_detect.transforms import TfBuffer, Transformer
from stalk_detect.utils import KillableSubscriber, Stalk
from stalk_detect.visualize import Visualizer


class DetectNode:
    @classmethod
    def __init__(cls):
        cls.model = MaskRCNN()
        cls.cv_bridge = CvBridge()

        cls.visualizer = Visualizer()

        cls.tf_buffer = TfBuffer()

        cls.get_stalk_service = rospy.Service('get_stalk', GetStalk, cls.find_stalk)

        try:
            camera_info = rospy.wait_for_message(CAMERA_INFO, CameraInfo, timeout=5)
        except rospy.ROSException:
            print(colored('Camera info not found, so camera is likely not running!', 'yellow'))

        # Count the number of total service calls (for visualization, etc)
        cls.call_index = -1

        # Count the number of images processed in the current service call
        cls.image_index = -1

        cls.driver_process = None

    @classmethod
    def run_detection(cls, image):
        '''
        Run the Mask R-CNN model on the given image

        Parameters
            image (sensor_msgs.msg.Image): The image to run the model on

        Returns
            masks (np.ndarray): The masks of the detected stalks
            output (np.ndarray): The output image of the model
            scores (np.ndarray): The scores of the detected stalks
        '''
        cv_image = cls.cv_bridge.imgmsg_to_cv2(image, desired_encoding='bgr8')

        # Run the model
        scores, bboxes, masks, output = cls.model.forward(cv_image)
        masks = masks.astype(np.uint8) * 255

        return masks, output, scores

    @classmethod
    def get_features(cls, masks) -> 'list[list[Pose2D]]':
        '''
        Get the center points going up each stalk

        Parameters
            masks (np.ndarray): The masks of the detected stalks

        Returns
            stalks_features (list[list[Pose2D]]): The center points of the stalks, for each stalk
        '''
        # Get the center points of the stalks
        stalks_features = []
        for mask in masks:
            # Ensure the mask has the minimum number of pixels
            if np.count_nonzero(mask) < MINIMUM_MASK_AREA:
                continue

            # Swap x and y in the mask
            mask = np.swapaxes(mask, 0, 1)
            nonzero = np.nonzero(mask)

            # Get the top and bottom height values of the stalk
            top_y, bottom_y = nonzero[1].min(), nonzero[1].max()

            stalk_features = [Pose2D(x=np.nonzero(mask[:, top_y])[0].mean(), y=top_y)]

            # For every 10 pixels, get the center point
            for y in range(top_y, bottom_y, 10):
                # Find the average x value for nonzero pixels at this y value
                y_values = np.nonzero(mask[:, y])[0]

                # If there are no y pixels, simply skip this value
                if len(y_values) > 0:
                    stalk_features.append(Pose2D(x=y_values.mean(), y=y))

            stalk_features.append(Pose2D(x=np.nonzero(mask[:, bottom_y])[0].mean(), y=bottom_y))

            stalks_features.append(stalk_features)

        return stalks_features

    @classmethod
    def determine_best_stalk(cls, positions) -> Point:
        '''
        Find the best stalk from a list of positions.

        Parameters
            positions (list): A list of positions to cluster and average

        Returns
            best_stalk (geometry_msgs.msg.Point): The best stalk found
        '''
        STORING_FAC = 100000.0  # Allowing for 2 decimal places

        original_positions = positions

        # Transform to list of 2D points
        positions = [[float(int(p.x * STORING_FAC)) + p.y, p.z] for p in positions]

        # Use a custom metric to allow 3-dimensional clustering
        def three_dim(a, b):
            return math.sqrt((a[0] / STORING_FAC - b[0] / STORING_FAC)**2 +
                             (a[0] % STORING_FAC - b[0] % STORING_FAC)**2 +
                             (a[1] - b[1])**2)

        clustering = DBSCAN(eps=0.1, min_samples=1, metric=three_dim).fit(positions)

        # Find the cluster with the most points
        best_cluster = None
        best_cluster_size = 0
        for cluster in set(clustering.labels_):
            cluster_size = np.count_nonzero(clustering.labels_ == cluster)
            if cluster_size > best_cluster_size:
                best_cluster_size = cluster_size
                best_cluster = cluster

        # Average the points in the best cluster
        best_stalk_average = np.mean(np.array(positions)[np.nonzero(clustering.labels_ == best_cluster)], axis=0)
        best_stalk_average = [round(best_stalk_average[0] / STORING_FAC, 2), best_stalk_average[0] % 1, best_stalk_average[1]]

        # Find the point in the best cluster closest to the average
        best_grasp_point: Point = Point(original_positions[0].x, original_positions[0].y, original_positions[0].z)
        best_stalk_dist = float('inf')
        for i in range(len(original_positions)):
            if clustering.labels_[i] == best_cluster:
                dist = math.sqrt((original_positions[i].x - best_stalk_average[0])**2 +
                                 (original_positions[i].y - best_stalk_average[1])**2 +
                                 (original_positions[i].z - best_stalk_average[2])**2)
                if dist < best_stalk_dist:
                    best_stalk_dist = dist
                    best_grasp_point = Point(original_positions[i].x, original_positions[i].y, original_positions[i].z)

        return best_grasp_point

    @classmethod
    def get_pcl(cls, image, depth_image, transformer):
        '''
        Get the point cloud

        Parameters
            image (sensor_msgs.msg.Image): The depth image

        Returns
            point_cloud (pcl PointCloud): The point cloud
        '''
        pcd = o3d.geometry.PointCloud.create_from_rgbd_image(
            o3d.geometry.RGBDImage.create_from_color_and_depth(
                o3d.geometry.Image(cls.cv_bridge.imgmsg_to_cv2(image, desired_encoding='bgr8')),
                o3d.geometry.Image(cls.cv_bridge.imgmsg_to_cv2(depth_image, desired_encoding='mono8')),
                depth_scale=DEPTH_SCALE, depth_trunc=DEPTH_TRUNC, convert_rgb_to_intensity=False),
            o3d.camera.PinholeCameraIntrinsic(
                transformer.width, transformer.height, transformer.intrinsic))

        return pcd

    @classmethod
    def transform_points(cls, stalks_features: 'list[list[Point]]', transformer: Transformer) -> 'list[list[Point]]':
        '''
        Transform the stalk features to the ground plane (using the robot base transform)

        Parameters
            stalk_features (list[list[Point]]): The center points of the stalks
            transformer (Transformer): The transformer

        Returns
            transformed_features (list[list[Point]]): The transformed features
        '''
        transformed_features = []
        for stalk in stalks_features:
            transformed_features.append([transformer.transform_cam_to_world(p) for p in stalk])

        return transformed_features

    @classmethod
    def ransac_ground_plane(cls, pointcloud):
        '''
        Find the ground plane using RANSAC

        Parameters
            pointcloud (o3d.geometry.PointCloud): The point cloud

        Returns
            plane (np.ndarray): The plane coefficients [A,B,C,D] for Ax+By+Cz+D=0
        '''
        # NOTE: Alternatively, use pointcloud.segment_plane from open3d

        A = 150
        B = 100
        C = 10
        colors = np.asarray(pointcloud.colors)
        # Find the brown points in the point cloud
        brown_points = np.array(pointcloud.points)[np.argwhere(np.logical_and(
            colors[2] < A, np.logical_and(abs(colors[0] - colors[1]) < B, np.maximum(colors[0], colors[1]) > C)))]

        # Find the plane using RANSAC
        plane = pyrsc.Plane()
        best_eq, best_inliers = plane.fit(brown_points, INLIER_THRESHOLD, MAX_RANSAC_ITERATIONS)

        return best_eq

    @classmethod
    def get_grasp_points_by_ransac(cls, stalks_features, pointcloud):
        '''
        Find a point closest to 1-3" from the ground plane for each stalk

        Parameters
            stalks_features (list[np.ndarray[Point]]): The center points of the stalks
            pointcloud (o3d.geometry.PointCloud): The point cloud

        Returns
            grasp_points ([[geometry_msgs.msg.Point, distance_to_ground]]):
                The best grasp points found for each stalk, and their distance to the ground plane
        '''
        # Find the ground plane
        plane = cls.ransac_ground_plane(pointcloud)

        # Find the point closest to 2" from the ground plane for each stalk
        # NOTE: This relies on the ground plane being horizontal. For non-horizontal, use point to plane distance
        goal_height = -plane[3] + 2
        grasp_points = []
        for stalk in stalks_features:
            best_point = min(stalk, key=lambda pt, goal_height=goal_height: abs(pt.z - goal_height))
            distance_to_ground = best_point.z - (-plane[3])
            grasp_points.append([best_point, distance_to_ground])

        return grasp_points

    @classmethod
    def combine_pointclouds(cls, pointclouds, transformers):
        '''
        Combine multiple point clouds into one using ICP and the given transformations

        Parameters
            pointclouds (list[o3d.geometry.PointCloud]): The point clouds

        Returns
            pointcloud (o3d.geometry.PointCloud): The combined point cloud
        '''
        # If ICP is needed
        transformations = []
        for pcl, transformer in zip(pointclouds[1:], transformers[1:]):
            # Find the predicted transformations between pcl and the first pointcloud
            trans_init = transformer.E @ np.linalg.inv(transformers[0].E)

            result = o3d.pipelines.registration.registration_icp(
                pcl, pointclouds[0], 0.05, trans_init,
                o3d.pipelines.registration.TransformationEstimationPointToPlane())
            transformations.append(result.transformation)

        # Combine the point clouds
        pointcloud = pointclouds[0]
        for pcl in pointclouds[1:]:
            # Transform the pointcloud to the frame of the first one if ICP is being used
            pcl.transform(transformations.pop())
            pointcloud.points.extend(pcl.points)

    @classmethod
    def run_realsense(cls):
        cls.driver_process = subprocess.Popen(DRIVER_COMMAND, stdout=subprocess.PIPE, shell=True, preexec_fn=os.setsid)

    @classmethod
    def stop_realsense(cls):
        if cls.driver_process is not None:
            os.killpg(os.getpgid(cls.driver_process.pid), signal.SIGTERM)
            cls.driver_process = None

    @classmethod
    def find_stalk(cls, req: GetStalkRequest) -> GetStalkResponse:
        '''
        Process a request to find the best stalk to grab

        Parameters
            req (GetStalkRequest): The request, with a number of frames to process and a timeout

        Returns
            GetStalkResponse: The response, with a message, the best stalk found, and # frames processed
        '''
        print('Received a request for {} frames with timeout {} seconds'.format(req.num_frames, req.timeout))

        # Run the realsense camera driver if needed
        if RUN_REALSENSE_ON_REQUEST:
            cls.run_realsense()

        try:
            camera_info = rospy.wait_for_message(CAMERA_INFO, CameraInfo, timeout=2)
            cls.camera_height = camera_info.height
            cls.camera_width = camera_info.width
        except rospy.ROSException:
            print(colored('Camera info not found, so camera is likely not running!', 'red'))
            return GetStalkResponse(success='ERROR', num_frames=0)

        cls.call_index += 1
        cls.image_index = -1
        start = rospy.get_rostime()
        frame_count: int = 0
        positions: list[Point] = []
        pcls = []
        transformers: list[Transformer] = []

        def image_depth_callback(image, depth_image):
            nonlocal frame_count, positions, pcls, transformers

            transformer = Transformer(cls.tf_buffer.get_tf_buffer())
            cls.image_index += 1

            # Run the model
            masks, output, scores = cls.run_detection(image)

            cv_depth_image = cls.cv_bridge.imgmsg_to_cv2(depth_image, desired_encoding="passthrough")
            cv_depth_image = np.array(cv_depth_image, dtype=np.float32)

            # region Grasp Point Finding
            stalks_features: list[list[Any]] = cls.get_features(masks)

            # Visualize the stalks features on the image
            if VISUALIZE:
                cv_image = cls.cv_bridge.imgmsg_to_cv2(image, desired_encoding='bgr8')
                features_image = cls.model.visualize(cv_image, output)
                features_image = features_image.astype(np.uint8)
                for stalk in stalks_features:
                    for point in stalk:
                        try:
                            cv.circle(features_image, (int(point.x), int(point.y)), 2, (0, 0, 255), -1)
                        except Exception as e:
                            print('ERROR HERE', e)
                cls.visualizer.publish_item('masks', features_image)

            # Add the depths to the stalk features
            for i, stalk in enumerate(stalks_features):
                for j in range(len(stalk)):
                    # Get the depth from at this point
                    stalks_features[i][j] = Point(x=cls.camera_width - stalk[j].x, y=cls.camera_height - stalk[j].y,
                                                  z=cv_depth_image[int(stalk[j].y), int(stalk[j].x)] / DEPTH_SCALE)

            # Transform the points
            stalks_features = cls.transform_points(stalks_features, transformer)

            # Turn these features into stalks
            stalks = []
            for stalk, score, mask in zip(stalks_features, scores, masks):
                new_stalk = Stalk(points=stalk, score=score, mask=mask)
                if new_stalk.is_valid():
                    stalks.append(new_stalk)

            if VISUALIZE:
                cls.visualizer.new_frame()
                cls.visualizer.publish_item('features', stalks_features, marker_color=[255, 0, 0])
                for stalk in stalks:
                    cls.visualizer.publish_item('stalk_lines', stalk, marker_color=[255, 255, 0], marker_size=0.01)

            if GRASP_POINT_ALGO == GraspPointFindingOptions.mask_only:
                for stalk in stalks:
                    stalk.set_grasp_point(min_height=min([p.z for p in stalk.points]))

            elif GRASP_POINT_ALGO == GraspPointFindingOptions.mask_projection:
                for stalk in stalks:
                    stalk.set_grasp_point(min_height=0)

            elif GRASP_POINT_ALGO == GraspPointFindingOptions.ransac_ground_plane:
                pointcloud = cls.get_pcl(image, depth_image, transformer)

                if VISUALIZE:
                    cls.visualizer.publish_item('pointcloud', pointcloud)

                grasp_points = cls.get_grasp_points_by_ransac(stalks_features, pointcloud)

            else:
                raise ValueError('Invalid grasp point finding algorithm')
            # endregion

            # Set the camera grasping points
            # TODO: Figure out a way to do this within the stalk without passing transformer
            for stalk in stalks:
                stalk.set_cam_grasp_point(transformer.transform_world_to_cam(stalk.grasp_point))

            # region Best Stalk Determination
            valid_stalks: list[Stalk] = [s for s in stalks if s.is_within_bounds()]

            if len(valid_stalks) == 0:
                rospy.logwarn('No stalks detected on frame {}'.format(cls.image_index))
                frame_count += 1
                cls.stop_realsense()
                return

            if BEST_STALK_ALGO == BestStalkOptions.largest:
                best_stalk = max(valid_stalks, key=lambda stalk: np.count_nonzero(stalk.mask))

            elif BEST_STALK_ALGO == BestStalkOptions.largest_favorable:
                # We have:
                #   mask_pixels: The number of pixels in the mask (order of x^2)
                #   mask_height: The average height of the mask (order of x)
                #   mask_width: The average width of the mask (order of x)
                #   distance_from_center: The distance from the center of the image (order of x)
                #   distance_from_camera: The distance from the camera (order of x)

                # So, we do a weighting function to decide the best stalk
                #   mask_pixels, mask_height, and mask_width can be multiplicative weights
                #   distance_from_center and distance_from_camera can be additive weights

                # Thus, we have:
                #   weight = mask_pixels * mask_height * mask_width -
                #       distance_from_center**3 - (distance_from_camera - optimal_distance)**3

                # Note that metrics like verticality of the masks should be taken care of by the segmentation

                # cv.imwrite('masks/{}.png'.format(cls.image_index), features_image)
                # print('IMAGE {}: y coords {}'.format(cls.image_index, [s.cam_grasp_point.y for s in valid_stalks]))

                graspable_stalks: list[Stalk] = []

                # Order by grasping points instead of masks, to account for possible sporadic points (negative means right)
                ordered_right_to_left: list[Stalk] = sorted(valid_stalks, key=lambda stalk: stalk.cam_grasp_point.y)

                # TODO: Create an imaginary "worst-case" stalk immediately outside the frame of the camera, to check if this first stalk is graspable

                # Iterate from the right to the left, eliminating stalks that are too close for the gripper to fit between
                graspable_stalks.append(ordered_right_to_left[0])
                for i in range(1, len(ordered_right_to_left)):
                    is_free = True
                    for j in range(0, i):
                        if ordered_right_to_left[i].cam_grasp_point.y - ordered_right_to_left[j].cam_grasp_point.y < GRIPPER_WIDTH and \
                                ordered_right_to_left[j].cam_grasp_point.x - ordered_right_to_left[i].cam_grasp_point.x < GRIPPER_LENGTH_PAST_STALK:
                            is_free = False
                            break

                    if is_free:
                        graspable_stalks.append(ordered_right_to_left[i])

                # For the remaining stalks, weight by width, confidence, and height
                stalk_weights = []
                for i ,stalk in enumerate(graspable_stalks):
                    # mask_pixels = np.count_nonzero(mask)
                    mask_height = np.ptp([p.z for p in stalk.points])

                    # For mask width, use the average width of the mask in the image
                    mask = np.swapaxes(stalk.mask, 0, 1)
                    nonzero = np.nonzero(mask)
                    top_y, bottom_y = nonzero[1].min(), nonzero[1].max()
                    widths = []

                    for y in range(top_y, bottom_y, 10):
                        x = nonzero[0][np.where(nonzero[1] == y)]
                        if len(x) > 0:
                            widths.append(np.ptp(x))

                    mask_width = np.mean(widths)

                    # distance_from_center = abs(np.mean([p.y for p in stalks[i].points]))
                    # distance_from_camera = valid_grasp_points[i].x

                    weight = (stalk.score ** 2) * mask_width * (math.sqrt(mask_height))

                    # print('Stalk index {} has weight {}, score {}, height {}, width {} y {}'.format(i, weight, stalks[i].score, mask_height, mask_width, stalk.cam_grasp_point.y))

                    stalk_weights.append(weight)

                best_stalk = graspable_stalks[np.argmax(stalk_weights)]

                if VISUALIZE:
                    cls.visualizer.publish_item('grasp_points', [best_stalk.grasp_point], marker_color=[255, 0, 255], marker_size=0.02)

            elif BEST_STALK_ALGO == BestStalkOptions.combine_pcls:
                positions += grasp_points
                pointcloud = cls.get_pcl(image, depth_image, transformer)
                pcls.append(pointcloud)
                transformers.append(transformer)

            if BEST_STALK_ALGO in [BestStalkOptions.largest, BestStalkOptions.largest_favorable]:
                positions.append(transformer.transform_world_to_cam(best_stalk.grasp_point))
            # endregion

            frame_count += 1

        # Setup the callback for the images and depth images
        cls.sub_images = KillableSubscriber(IMAGE_TOPIC, Image)
        cls.sub_depth = KillableSubscriber(DEPTH_TOPIC, Image)
        ts = ApproximateTimeSynchronizer(
            [cls.sub_images, cls.sub_depth], queue_size=5, slop=0.2)
        ts.registerCallback(image_depth_callback)

        # Continue until enough frames have been gathered, or the timeout has been reached
        while frame_count < req.num_frames and (rospy.get_rostime() - start).to_sec() < req.timeout:
            rospy.sleep(0.1)

        # Unregister the callback
        cls.sub_depth.unregister()
        cls.sub_images.unregister()
        del ts

        #  Cluster the positions, find the best one, average it
        if len(positions) == 0:
            print(colored('No stalks detected for this service request, requesting a REPOSITION', 'red'))
            cls.stop_realsense()
            return GetStalkResponse(success='REPOSITION')

        # Option 1 or 2.1: Simply find the consensus among the individual decisions
        if BEST_STALK_ALGO in [BestStalkOptions.largest, BestStalkOptions.largest_favorable]:
            best_stalk = cls.determine_best_stalk(positions)

        elif BEST_STALK_ALGO == BestStalkOptions.combine_pcls:
            # Combine point clouds with the first one
            single_pcl = cls.combine_pointclouds(pcls, transformers)

            # Find the grasp points for this point cloud, which is in the frame of the first point cloud
            grasp_points = cls.get_grasp_points_by_ransac(positions, single_pcl, transformers[0])

            best_stalk = cls.determine_best_stalk(grasp_points)

        print(colored('Found stalk at robot frame ({}, {}, {})'.format(
            best_stalk.x, best_stalk.y, best_stalk.z), 'green'))

        # Stop the camera driver
        cls.stop_realsense()
        return GetStalkResponse(success='DONE', position=best_stalk, num_frames=cls.image_index)


if __name__ == '__main__':
    print('Starting stalk_detect node.\n\tUsing grasp point algorithm: {}\n\tUsing best stalk algorithm: {}'.format(
        GRASP_POINT_ALGO.name, BEST_STALK_ALGO.name))

    rospy.init_node('stalk_detect')

    detect_node = DetectNode()

    print(colored('Loaded model. Waiting for service calls...', 'green'))

    rospy.spin()
